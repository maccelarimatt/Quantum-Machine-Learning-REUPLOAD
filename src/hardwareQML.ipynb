{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4a4b4e",
   "metadata": {},
   "source": [
    "# Running a Machine Learning Model on Quantum Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68098596",
   "metadata": {},
   "source": [
    "### Imports and Environment Setup\n",
    "This cell imports all necessary libraries for data handling (`pandas`, `NumPy`), preprocessing and modelling (`scikit-learn`), plotting (`matplotlib`), and quantum machine learning (`Qiskit` and its machine-learning extensions). It also ensures the `Heatmaps/` directory exists for saving later figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd036fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------ Import Required Python Packages ------------\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "##------------ Import Required Machine Learning Packages ------------\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "\n",
    "##------------ Import Required Qiskit Packages ------------\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_machine_learning.kernels import FidelityStatevectorKernel\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit import transpile\n",
    "from qiskit_ibm_runtime       import QiskitRuntimeService, SamplerV2 as Sampler\n",
    "\n",
    "##------------ Create/Find OS Directory to Save the Heatmap Results ------------\n",
    "os.makedirs('../Heatmaps', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1e359",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "This cell reads the raw dataset (`bots_vs_users.csv`) into a DataFrame, drops features with over 75 % missing values, imputes and flags missing entries, encodes boolean and categorical variables, removes duplicate rows, filters out features with very low variance, and finally imputes and standardises the cleaned feature matrix in preparation for PCA and modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f474b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2651 duplicate rows\n",
      "Kept 74 features after low-variance filtering\n"
     ]
    }
   ],
   "source": [
    "#----------- Load Data into DataFrame -----------\n",
    "df = pd.read_csv('../Dataset/bots_vs_users.csv')    # Read CSV file into pandas DataFrame\n",
    "\n",
    "#----------- Drop Very Sparse Columns (>75% missing) -----------\n",
    "missing_frac = df.isnull().mean().sort_values(ascending=False)    # Compute fraction of missing values per column\n",
    "high_na      = missing_frac[missing_frac > 0.75].index.tolist()   # Identify columns with >75% NaNs\n",
    "df           = df.drop(columns=high_na)                           # Remove those sparse columns\n",
    "\n",
    "\n",
    "#----------- Numeric Imputation & Flags -----------\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns.drop('target')   # Numeric columns excluding target\n",
    "df[num_cols] = df[num_cols].fillna(0)                                             # Impute NaNs with zero\n",
    "for c in num_cols:\n",
    "    df[c + '_was_na'] = (df[c] == 0).astype(int)                                  # Add binary flag for imputed entries\n",
    "\n",
    "\n",
    "#----------- Boolean Mapping & One-Hot Encoding for Categoricals -----------\n",
    "bool_cols = [\n",
    "    c for c in df.columns\n",
    "    if df[c].dtype == 'object' and set(df[c].dropna().unique()) <= {'True','False'}\n",
    "]                                                                                 # Detect True/False columns\n",
    "for c in bool_cols:\n",
    "    df[c] = df[c].map({'True':1,'False':0})                                       # Map boolean strings to 0/1\n",
    "cat_cols = df.select_dtypes(include=['object']).columns                           # Remaining categorical columns\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)                        # One-hot encode, drop first level\n",
    "\n",
    "#----------- Drop Duplicate Rows -----------\n",
    "df = df.reset_index(drop=True)                       # Reset index after modifications\n",
    "y_full = df['target']                                # Save target series for later\n",
    "before = len(df)                                     # Count rows before deduplication\n",
    "df = df.drop_duplicates()                            # Remove exact duplicate records\n",
    "print(f\"Dropped {before - len(df)} duplicate rows\")  # Report number of duplicates removed\n",
    "\n",
    "\n",
    "#----------- Low-Variance Feature Filter (<1%) -----------\n",
    "X_full = df.drop(columns=['target'])                              # Separate features\n",
    "sel    = VarianceThreshold(threshold=0.01)                        # Initialise low-variance selector\n",
    "X_sel  = sel.fit_transform(X_full)                                # Filter out features with <1% variance\n",
    "kept   = X_full.columns[sel.get_support()]                        # List of retained feature names\n",
    "print(f\"Kept {len(kept)} features after low-variance filtering\")   # Report retained feature count\n",
    "\n",
    "#----------- Rebuild Cleaned DataFrame & Reattach Target -----------\n",
    "df_clean = pd.DataFrame(X_sel, columns=kept)                      # Construct DataFrame from filtered features\n",
    "df_clean['target'] = y_full.loc[df.index].values                  # Reattach the target values\n",
    "\n",
    "#----------- Final Split into Features/Target, Imputation, Normalization & Standardization -----------\n",
    "X = df_clean.drop(columns=['target'])                             # Final feature matrix\n",
    "y = df_clean['target']                                            # Final target vector\n",
    "\n",
    "# Impute any remaining missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')                          # Mean-value imputer\n",
    "X_imp   = imputer.fit_transform(X)                                # Impute features\n",
    "\n",
    "# Normalise features to [0,1]\n",
    "norm_scaler = MinMaxScaler()                                       # Min–Max normaliser\n",
    "X_norm      = norm_scaler.fit_transform(X_imp)                     # Scale each feature into [0,1]\n",
    "\n",
    "# Standardise to zero mean and unit variance\n",
    "std_scaler = StandardScaler()                                      # Z-score standardiser\n",
    "X_scaled  = std_scaler.fit_transform(X_norm)                       # Transform normalised data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda9fa5",
   "metadata": {},
   "source": [
    "### PCA reduction & train/test split\n",
    "\n",
    "In this section we:\n",
    "\n",
    "1. Apply **Principal Component Analysis** (PCA) to reduce the pre-scaled feature matrix down to `n_components = 3` dimensions.  \n",
    "2. **Subsample** the reduced data to 50 examples (while preserving class balance).  \n",
    "3. Perform an **80/20 train/test split** on that 50-sample subset, producing `X_train`, `X_test`, `y_train` and `y_test` for downstream kernel estimation and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- PCA reduction & data splitting ------------\n",
    "n_components = 3                                            # Number of principal components to retain\n",
    "pca          = PCA(n_components=n_components)               # Initialise PCA transformer\n",
    "X_red        = pca.fit_transform(X_scaled)                  # Fit PCA on scaled data and reduce dimensionality\n",
    "\n",
    "# Subsample fewer points to begin with\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X_red,                                                  # Reduced feature matrix\n",
    "    y,                                                      # Labels\n",
    "    train_size=80,                                          # Keep only 50 samples\n",
    "    stratify=y,                                             # Preserve class balance\n",
    "    random_state=42                                         # Fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "# Now do the 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sub,                                                  # Subsampled features\n",
    "    y_sub,                                                  # Subsampled labels\n",
    "    test_size=0.2,                                          # Allocate 20% for test set\n",
    "    stratify=y_sub,                                         # Maintain class proportions\n",
    "    random_state=42                                         # Fixed seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdfb14",
   "metadata": {},
   "source": [
    "### Feature map definition & hardware sampler setup\n",
    "\n",
    "Here we prepare everything needed to run on real IBM Quantum hardware:\n",
    "\n",
    "1. Define a **ZZFeatureMap** with `reps=1` to encode each real feature vector as a parameterised quantum circuit.  \n",
    "2. Instantiate the **QiskitRuntimeService** (using your stored API token) and pick the **least-busy real device** capable of handling `n_components` qubits.  \n",
    "3. Wrap that device in a **Sampler** primitive so we can submit parameterised circuits and receive measurement outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79a54dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattm\\AppData\\Local\\Temp\\ipykernel_4916\\1054487773.py:6: DeprecationWarning: The \"ibm_quantum\" channel option is deprecated and will be sunset on 1 July. After this date, \"ibm_cloud\" and \"local\" will be the only valid channels. For information on migrating to the new IBM Quantum Platform on the \"ibm_cloud\" channel, review the migration guide https://quantum.cloud.ibm.com/docs/migration-guides/classic-iqp-to-cloud-iqp .\n",
      "  service = QiskitRuntimeService()                                      # Initialise QiskitRuntimeService (loads your saved IBMQ token)\n"
     ]
    }
   ],
   "source": [
    "#----------- Feature Map & Hardware Sampler Setup --------------------------------\n",
    "# 1.) feature map\n",
    "feature_map = ZZFeatureMap(feature_dimension=n_components, reps=1)    # Create a ZZFeatureMap to encode each n_components-dimensional vector, with one repetition\n",
    "\n",
    "# 2.) get least-busy real device\n",
    "service = QiskitRuntimeService()                                      # Initialise QiskitRuntimeService (loads your saved IBMQ token)\n",
    "backend = service.least_busy(\n",
    "    simulator=False,                                                  # Exclude simulators; only consider real devices\n",
    "    operational=True,                                                 # Filter for devices currently operational\n",
    "    min_num_qubits=n_components                                       # Ensure the device has at least n_components qubits\n",
    ")\n",
    "\n",
    "# 3.) sampler primitive on that backend\n",
    "sampler = Sampler(mode=backend)                                       # Wrap the chosen real device in a Sampler primitive for runtime execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65dd02",
   "metadata": {},
   "source": [
    "### Fidelity circuit builder & kernel‐matrix function\n",
    "\n",
    "Two helper functions:\n",
    "\n",
    "- `fidelity_circuit(x, y)`:  \n",
    "  - Prepares the state \\|ϕ(x)⟩ via the feature map, then “unprepares” \\|ϕ(y)⟩ by appending the inverse circuit,  \n",
    "  - Measures all qubits to estimate the overlap (fidelity) between ∣ϕ(x)⟩ and ∣ϕ(y)⟩.\n",
    "\n",
    "- `compute_kernel_matrix(X1, X2, shots=128)`:  \n",
    "  1. Builds all pairwise fidelity circuits between samples in `X1` and `X2`.  \n",
    "  2. Transpiles & maps them to the chosen hardware.  \n",
    "  3. Runs them via the `Sampler` primitive.  \n",
    "  4. Extracts the probability of the all-zero outcome (∣0…0⟩) from each circuit’s counts.  \n",
    "  5. Returns an |X1|×|X2| NumPy array of these fidelity estimates, forming the quantum kernel matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbbbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Fidelity Circuit & Kernel Computation Functions -----------------------\n",
    "\n",
    "def fidelity_circuit(x, y):\n",
    "    qc            = QuantumCircuit(n_components, n_components)         # Create circuit with n_components qubits and classical bits\n",
    "    # prepare |φ(x)⟩\n",
    "    param_dict_x  = dict(zip(feature_map.parameters, x))               # Map feature map parameters to the x-vector\n",
    "    prep_inst     = feature_map.assign_parameters(param_dict_x) \\\n",
    "                          .to_instruction()                            # Build the parameterised feature-map instruction\n",
    "    qc.append(prep_inst, range(n_components))                          # Append prepare(φ(x)) on all qubits\n",
    "\n",
    "    # un-prepare ⟨φ(y)| via inverse\n",
    "    param_dict_y  = dict(zip(feature_map.parameters, y))               # Map feature map parameters to the y-vector\n",
    "    unprep_inst   = feature_map.assign_parameters(param_dict_y) \\\n",
    "                          .to_instruction().inverse()                  # Build the inverse instruction for φ(y)\n",
    "    qc.append(unprep_inst, range(n_components))                        # Append unprepare(φ(y)) on all qubits\n",
    "\n",
    "    qc.measure_all()                                                   # Measure all qubits into classical bits\n",
    "    return qc                                                          # Return the constructed fidelity circuit\n",
    "\n",
    "def compute_kernel_matrix(X1, X2, shots=128):\n",
    "    # 1) Build all pairwise fidelity circuits\n",
    "    circs     = [fidelity_circuit(x, y) for x in X1 for y in X2]        # List of |X1|×|X2| circuits\n",
    "    # 2) Transpile & map to the chosen backend\n",
    "    circs     = transpile(circs, backend=backend, optimization_level=3) # Optimise circuits for hardware\n",
    "    # 3) Execute on real device via Sampler primitive\n",
    "    job       = sampler.run(circs, shots=shots)                         # Submit batch job with specified shots\n",
    "    results   = job.result()                                            # Retrieve PrimitiveResult iterable\n",
    "    # 4) Extract P(|0…0⟩) from each measurement result\n",
    "    probs     = [\n",
    "        r.data.meas.get_counts().get('0'*n_components, 0) / shots       # Probability of the all-zero outcome\n",
    "        for r in results\n",
    "    ]\n",
    "    # 5) Reshape into kernel matrix\n",
    "    return np.array(probs).reshape(len(X1), len(X2))                    # Return an |X1|×|X2| NumPy array of fidelities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fce6ae",
   "metadata": {},
   "source": [
    "### Kernel evaluation & classical SVM training\n",
    "\n",
    "Using our hardware-computed Gram matrices:\n",
    "\n",
    "1. Compute `K_train = compute_kernel_matrix(X_train, X_train)` and fit an **SVM with a precomputed kernel** on the training data.  \n",
    "2. Compute `K_test = compute_kernel_matrix(X_test, X_train)` and report the **test accuracy** of the precomputed-kernel SVM on the held-out split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99097b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "#----------- Kernel Evaluation & Classical SVM Training -----------------------------\n",
    "\n",
    "# 1.) Train-kernel\n",
    "K_train = compute_kernel_matrix(X_train, X_train)         # Compute Gram matrix for training data on hardware\n",
    "svc     = SVC(kernel='precomputed')                       # Initialise SVM using precomputed kernel\n",
    "svc.fit(K_train, y_train)                                 # Train SVM on the training Gram matrix\n",
    "\n",
    "# 2.) Test-kernel + accuracy\n",
    "K_test  = compute_kernel_matrix(X_test, X_train)          # Compute Gram matrix between test and training data\n",
    "print(\"Test accuracy:\", svc.score(K_test, y_test))        # Evaluate SVM on test Gram matrix and print accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc6121",
   "metadata": {},
   "source": [
    "### Saving the kernel heatmap\n",
    "\n",
    "Finally, we visualise—and **save**—the training-kernel matrix:\n",
    "\n",
    "- Generate a Matplotlib heatmap of `K_train`.  \n",
    "- Save the figure to `../Benchmarks/kernel_train_n{n_components}_hardware.png` (creating the folder if necessary).  \n",
    "- Close the figure without rendering it inline in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da003f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Plot & Save Training Kernel Heatmap -------------------\n",
    "plt.figure(figsize=(6,6))                                         # Create a square figure of size 6×6 inches\n",
    "plt.imshow(K_train, cmap='viridis')                               # Render the training kernel matrix as a heatmap\n",
    "plt.title(\"Quantum Kernel Matrix (Train)\")                        # Set the plot title\n",
    "plt.xlabel(\"Train index\")                                         # Label the x-axis\n",
    "plt.ylabel(\"Train index\")                                         # Label the y-axis\n",
    "plt.colorbar(label=\"P(|0…0⟩)\")                                    # Add colourbar showing the probability of |0…0⟩\n",
    "\n",
    "# Save the figure to the Benchmarks folder\n",
    "out_path = f\"../Heatmaps/kernel_train_n{n_components}_hardware.png\"      # Build filename including current PCA dimension\n",
    "plt.savefig(out_path, bbox_inches='tight')                        # Export the figure without extra whitespace\n",
    "plt.close()                                                       # Close the figure to suppress inline display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
