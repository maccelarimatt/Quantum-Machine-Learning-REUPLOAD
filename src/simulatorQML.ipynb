{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50db8c0",
   "metadata": {},
   "source": [
    "# Running a Machine Learning Model in a QSVM and Classic SVM Simulator With Comparasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013b98c",
   "metadata": {},
   "source": [
    "### Imports and Environment Setup\n",
    "This cell imports all necessary libraries for data handling (`pandas`, `NumPy`), preprocessing and modelling (`scikit-learn`), plotting (`matplotlib`), and quantum machine learning (`Qiskit` and its machine-learning extensions). It also ensures the `Heatmaps/` directory exists for saving later figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1087dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------ Import Required Python Packages ------------\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "##------------ Import Required Machine Learning Packages ------------\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC as ClassicalSVC\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "\n",
    "##------------ Import Required Qiskit Packages ------------\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_machine_learning.kernels import FidelityStatevectorKernel\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "##------------ Create/Find OS Directory to Save the Heatmap Results ------------\n",
    "os.makedirs('../Heatmaps', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74c856",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "This cell reads the raw dataset (`bots_vs_users.csv`) into a DataFrame, drops features with over 75 % missing values, imputes and flags missing entries, encodes boolean and categorical variables, removes duplicate rows, filters out features with very low variance, and finally imputes and standardises the cleaned feature matrix in preparation for PCA and modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc736aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2651 duplicate rows\n",
      "Kept 74 features after low-variance filtering\n"
     ]
    }
   ],
   "source": [
    "#----------- Load Data into DataFrame -----------\n",
    "df = pd.read_csv('../Dataset/bots_vs_users.csv')    # Read CSV file into pandas DataFrame\n",
    "\n",
    "#----------- Drop Very Sparse Columns (>75% missing) -----------\n",
    "missing_frac = df.isnull().mean().sort_values(ascending=False)    # Compute fraction of missing values per column\n",
    "high_na      = missing_frac[missing_frac > 0.75].index.tolist()   # Identify columns with >75% NaNs\n",
    "df           = df.drop(columns=high_na)                           # Remove those sparse columns\n",
    "\n",
    "\n",
    "#----------- Numeric Imputation & Flags -----------\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns.drop('target')   # Numeric columns excluding target\n",
    "df[num_cols] = df[num_cols].fillna(0)                                             # Impute NaNs with zero\n",
    "for c in num_cols:\n",
    "    df[c + '_was_na'] = (df[c] == 0).astype(int)                                  # Add binary flag for imputed entries\n",
    "\n",
    "\n",
    "#----------- Boolean Mapping & One-Hot Encoding for Categoricals -----------\n",
    "bool_cols = [\n",
    "    c for c in df.columns\n",
    "    if df[c].dtype == 'object' and set(df[c].dropna().unique()) <= {'True','False'}\n",
    "]                                                                                 # Detect True/False columns\n",
    "for c in bool_cols:\n",
    "    df[c] = df[c].map({'True':1,'False':0})                                       # Map boolean strings to 0/1\n",
    "cat_cols = df.select_dtypes(include=['object']).columns                           # Remaining categorical columns\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)                        # One-hot encode, drop first level\n",
    "\n",
    "#----------- Drop Duplicate Rows -----------\n",
    "df = df.reset_index(drop=True)                       # Reset index after modifications\n",
    "y_full = df['target']                                # Save target series for later\n",
    "before = len(df)                                     # Count rows before deduplication\n",
    "df = df.drop_duplicates()                            # Remove exact duplicate records\n",
    "print(f\"Dropped {before - len(df)} duplicate rows\")  # Report number of duplicates removed\n",
    "\n",
    "\n",
    "#----------- Low-Variance Feature Filter (<1%) -----------\n",
    "X_full = df.drop(columns=['target'])                              # Separate features\n",
    "sel    = VarianceThreshold(threshold=0.01)                        # Initialise low-variance selector\n",
    "X_sel  = sel.fit_transform(X_full)                                # Filter out features with <1% variance\n",
    "kept   = X_full.columns[sel.get_support()]                        # List of retained feature names\n",
    "print(f\"Kept {len(kept)} features after low-variance filtering\")   # Report retained feature count\n",
    "\n",
    "#----------- Rebuild Cleaned DataFrame & Reattach Target -----------\n",
    "df_clean = pd.DataFrame(X_sel, columns=kept)                       # Construct DataFrame from filtered features\n",
    "df_clean['target'] = y_full.loc[df.index].values                  # Reattach the target values\n",
    "\n",
    "#----------- Final Split into Features/Target, Imputation, Normalization & Standardization -----------\n",
    "X = df_clean.drop(columns=['target'])                             # Final feature matrix\n",
    "y = df_clean['target']                                            # Final target vector\n",
    "\n",
    "# Impute any remaining missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')                          # Mean-value imputer\n",
    "X_imp   = imputer.fit_transform(X)                                # Impute features\n",
    "\n",
    "# Normalise features to [0,1]\n",
    "norm_scaler = MinMaxScaler()                                       # Min–Max normaliser\n",
    "X_norm      = norm_scaler.fit_transform(X_imp)                     # Scale each feature into [0,1]\n",
    "\n",
    "# Standardise to zero mean and unit variance\n",
    "std_scaler = StandardScaler()                                      # Z-score standardiser\n",
    "X_scaled  = std_scaler.fit_transform(X_norm)                       # Transform normalised data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a30d7",
   "metadata": {},
   "source": [
    "### Experiment Parameter Configuration\n",
    "Here we define the grid of experimental conditions: a list of PCA component counts (`n_components_list`) and dataset sample sizes (`sample_sizes`). We also initialise an empty list (`results`) to collect accuracy metrics for each run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9075d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Initialise Parameters -----------\n",
    "n_components_list = [2, 3, 4, 5, 6, 7, 8]   # Matrix used for adjusting the PCA values (Fits the required range 2-4)\n",
    "sample_sizes      = [10, 100, 1000, 3000]   # Matrix used for adjusting the Sample size used for processing (Fits the required 100 samples)\n",
    "results = []                                # Used to store each of the accuracies from all of the n components and samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0031472",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation Loop\n",
    "This cell implements the core experiment. For each combination of PCA dimension and sample size, it:\n",
    "1. Applies PCA to reduce dimensionality.\n",
    "2. Subsamples and stratifies the data, then splits into training and test sets.\n",
    "3. Trains a quantum SVM (`QSVC` with `ZZFeatureMap` + `FidelityStatevectorKernel`) and records its test accuracy.\n",
    "4. Trains a classical RBF-kernel SVM on the same splits and records its test accuracy.\n",
    "5. Computes the quantum kernel matrix on the training set, plots it as a heatmap, and saves the figure under `Heatmaps/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdbd995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=2, m=10 → QSVC: acc=1.00, Support Vectors=6, margin=0.591 │ SVC: acc=1.00, Support Vectors=5, margin=0.707\n",
      "n=2, m=100 → QSVC: acc=0.80, Support Vectors=42, margin=0.093 │ SVC: acc=0.85, Support Vectors=35, margin=0.101\n",
      "n=2, m=1000 → QSVC: acc=0.78, Support Vectors=376, margin=0.010 │ SVC: acc=0.90, Support Vectors=234, margin=0.016\n",
      "n=2, m=3000 → QSVC: acc=0.78, Support Vectors=1084, margin=0.003 │ SVC: acc=0.90, Support Vectors=631, margin=0.006\n",
      "n=3, m=10 → QSVC: acc=1.00, Support Vectors=7, margin=0.683 │ SVC: acc=1.00, Support Vectors=5, margin=0.707\n",
      "n=3, m=100 → QSVC: acc=0.80, Support Vectors=47, margin=0.119 │ SVC: acc=0.90, Support Vectors=33, margin=0.110\n",
      "n=3, m=1000 → QSVC: acc=0.84, Support Vectors=334, margin=0.016 │ SVC: acc=0.91, Support Vectors=221, margin=0.020\n",
      "n=3, m=3000 → QSVC: acc=0.84, Support Vectors=986, margin=0.005 │ SVC: acc=0.90, Support Vectors=608, margin=0.007\n",
      "n=4, m=10 → QSVC: acc=1.00, Support Vectors=8, margin=0.706 │ SVC: acc=1.00, Support Vectors=6, margin=0.707\n",
      "n=4, m=100 → QSVC: acc=0.85, Support Vectors=58, margin=0.156 │ SVC: acc=0.90, Support Vectors=35, margin=0.108\n",
      "n=4, m=1000 → QSVC: acc=0.84, Support Vectors=355, margin=0.026 │ SVC: acc=0.91, Support Vectors=225, margin=0.020\n",
      "n=4, m=3000 → QSVC: acc=0.85, Support Vectors=959, margin=0.009 │ SVC: acc=0.91, Support Vectors=596, margin=0.008\n",
      "n=5, m=10 → QSVC: acc=1.00, Support Vectors=8, margin=0.704 │ SVC: acc=1.00, Support Vectors=6, margin=0.707\n",
      "n=5, m=100 → QSVC: acc=0.85, Support Vectors=75, margin=0.182 │ SVC: acc=0.90, Support Vectors=37, margin=0.109\n",
      "n=5, m=1000 → QSVC: acc=0.86, Support Vectors=430, margin=0.038 │ SVC: acc=0.91, Support Vectors=223, margin=0.021\n",
      "n=5, m=3000 → QSVC: acc=0.88, Support Vectors=1009, margin=0.014 │ SVC: acc=0.91, Support Vectors=592, margin=0.008\n",
      "n=6, m=10 → QSVC: acc=1.00, Support Vectors=8, margin=0.684 │ SVC: acc=1.00, Support Vectors=6, margin=0.707\n",
      "n=6, m=100 → QSVC: acc=0.85, Support Vectors=78, margin=0.197 │ SVC: acc=0.85, Support Vectors=35, margin=0.125\n",
      "n=6, m=1000 → QSVC: acc=0.87, Support Vectors=570, margin=0.052 │ SVC: acc=0.91, Support Vectors=230, margin=0.024\n",
      "n=6, m=3000 → QSVC: acc=0.90, Support Vectors=1301, margin=0.021 │ SVC: acc=0.92, Support Vectors=596, margin=0.010\n",
      "n=7, m=10 → QSVC: acc=1.00, Support Vectors=8, margin=0.707 │ SVC: acc=1.00, Support Vectors=7, margin=0.707\n",
      "n=7, m=100 → QSVC: acc=0.85, Support Vectors=78, margin=0.205 │ SVC: acc=0.90, Support Vectors=37, margin=0.126\n",
      "n=7, m=1000 → QSVC: acc=0.88, Support Vectors=670, margin=0.064 │ SVC: acc=0.93, Support Vectors=210, margin=0.028\n",
      "n=7, m=3000 → QSVC: acc=0.90, Support Vectors=1669, margin=0.028 │ SVC: acc=0.93, Support Vectors=537, margin=0.012\n",
      "n=8, m=10 → QSVC: acc=1.00, Support Vectors=8, margin=0.706 │ SVC: acc=1.00, Support Vectors=7, margin=0.707\n",
      "n=8, m=100 → QSVC: acc=0.85, Support Vectors=78, margin=0.212 │ SVC: acc=0.95, Support Vectors=38, margin=0.144\n",
      "n=8, m=1000 → QSVC: acc=0.88, Support Vectors=683, margin=0.073 │ SVC: acc=0.94, Support Vectors=204, margin=0.031\n",
      "n=8, m=3000 → QSVC: acc=0.91, Support Vectors=1888, margin=0.034 │ SVC: acc=0.94, Support Vectors=527, margin=0.013\n"
     ]
    }
   ],
   "source": [
    "#----------- Experimental Loop over PCA Dimensions and Sample Sizes -----------\n",
    "for n_components in n_components_list:\n",
    "    # 1.) PCA reduction\n",
    "    pca   = PCA(n_components=n_components)                 # Initialise PCA transformer to reduce to n_components\n",
    "    X_red = pca.fit_transform(X_scaled)                    # Apply PCA to the scaled feature matrix\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        #----------- Subsampling and Train/Test Split -----------\n",
    "        # 2a.) Subsample\n",
    "        X_sub, _, y_sub, _ = train_test_split(\n",
    "            X_red, y,\n",
    "            train_size=sample_size,\n",
    "            stratify=y,\n",
    "            random_state=42\n",
    "        )                                                   # Draw stratified subsample of size sample_size\n",
    "\n",
    "        # 2b.) Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_sub, y_sub,\n",
    "            test_size=0.2,\n",
    "            stratify=y_sub,\n",
    "            random_state=42\n",
    "        )                                                   # Split subsample into 80/20 stratified train and test sets\n",
    "        \n",
    "        #----------- Quantum Support Vector Classifier -----------\n",
    "        # 3.) Quantum QSVC\n",
    "        feature_map    = ZZFeatureMap(feature_dimension=n_components, reps=2)   # Create ZZ feature map circuit (2 reps)\n",
    "        quantum_kernel = FidelityStatevectorKernel(feature_map=feature_map)     # Build fidelity kernel from feature map\n",
    "        t0 = time.perf_counter()\n",
    "        qsvc           = QSVC(quantum_kernel=quantum_kernel)                    # Initialise QSVC with quantum kernel\n",
    "        qsvc.fit(X_train, y_train)                                              # Train QSVC on training data\n",
    "        quantum_acc = qsvc.score(X_test, y_test)                                # Evaluate QSVC accuracy on test set\n",
    "        quantum_train_time = time.perf_counter() - t0\n",
    "        sv_idx_q         = qsvc.support_                    # indices of SVs in X_train\n",
    "        alpha_q          = qsvc.dual_coef_[0]               # dual coefficients (y*alpha)\n",
    "        y_sv_q           = y_train.iloc[sv_idx_q].to_numpy() if hasattr(y_train, 'iloc') else y_train[sv_idx_q]\n",
    "        # compute quantum margin\n",
    "        K_sv_q           = quantum_kernel.evaluate(x_vec=X_train[sv_idx_q])\n",
    "        margin_sq_q      = (alpha_q * y_sv_q) @ K_sv_q @ (alpha_q * y_sv_q)\n",
    "        margin_q         = 1.0 / np.sqrt(margin_sq_q)\n",
    "        \n",
    "        #----------- Classical Support Vector Classifier Baseline -----------\n",
    "        # 4.) Classical SVM (RBF)\n",
    "        classical_svc = ClassicalSVC(kernel='rbf', gamma='scale')             # Instantiate classical SVM with RBF kernel\n",
    "        t1 = time.perf_counter()\n",
    "        classical_svc.fit(X_train, y_train)                                   # Train classical SVM on training data\n",
    "        classical_acc = classical_svc.score(X_test, y_test)                   # Evaluate classical SVM accuracy on test set\n",
    "        classical_train_time = time.perf_counter() - t1   \n",
    "        sv_idx_c         = classical_svc.support_\n",
    "        alpha_c          = classical_svc.dual_coef_[0]\n",
    "        y_sv_c           = y_train.iloc[sv_idx_c].to_numpy() if hasattr(y_train, 'iloc') else y_train[sv_idx_c]\n",
    "        # compute classical margin using RBF kernel with gamma=1/scale (same as SVC default)\n",
    "        K_sv_c           = rbf_kernel(X_train[sv_idx_c], X_train[sv_idx_c])\n",
    "        margin_sq_c      = (alpha_c * y_sv_c) @ K_sv_c @ (alpha_c * y_sv_c)\n",
    "        margin_c         = 1.0 / np.sqrt(margin_sq_c)\n",
    "        \n",
    "        #----------- Print timing & accuracies -----------\n",
    "        print(f\"n={n_components}, m={sample_size} → \"\n",
    "          f\"QSVC: acc={quantum_acc:.2f}, Support Vectors={len(sv_idx_q)}, margin={margin_q:.3f} │ \"\n",
    "          f\"SVC: acc={classical_acc:.2f}, Support Vectors={len(sv_idx_c)}, margin={margin_c:.3f}\")\n",
    "        \n",
    "        #----------- Record Results -----------\n",
    "        results.append({                                                       # Append metrics dictionary to results list\n",
    "            'n_components'      : n_components,                                #   PCA component count\n",
    "            'sample_size'       : sample_size,                                 #   Number of samples used\n",
    "            'quantum_accuracy'  : quantum_acc,                                 #   QSVC test accuracy\n",
    "            'classical_accuracy': classical_acc                                #   Classical SVM test accuracy\n",
    "        })\n",
    "        \n",
    "        #----------- Kernel Matrix Computation and Heatmap Saving -----------\n",
    "        # 5.) Printing Heatmaps\n",
    "        K_train = quantum_kernel.evaluate(x_vec=X_train)                     # Compute quantum kernel matrix on training set\n",
    "        plt.figure(figsize=(4,4))                                            # Create new figure for heatmap\n",
    "        plt.imshow(K_train, cmap='viridis')                                  # Display kernel matrix as a heatmap\n",
    "        plt.title(f\"Kernel (n={n_components}, size={sample_size})\")          # Set heatmap title\n",
    "        plt.xlabel(\"train idx\")                                              # Label x-axis\n",
    "        plt.ylabel(\"train idx\")                                              # Label y-axis\n",
    "        plt.colorbar(label=\"K value\")                                        # Add colorbar with label\n",
    "        fname = f\"../Heatmaps/kernel_n{n_components}_size{sample_size}_sim.png\"     # Define output filename\n",
    "        plt.savefig(fname, bbox_inches='tight')                              # Save heatmap without extra margins\n",
    "        plt.close()                                                          # Close figure to prevent inline display\n",
    "\n",
    "        X_var  = X_train.var()                                                \n",
    "        gamma  = 1.0 / (X_train.shape[1] * X_var)                             \n",
    "        K_train_cl = rbf_kernel(X_train, X_train, gamma=gamma)                # now gamma is a float\n",
    "\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(K_train_cl, cmap='viridis')\n",
    "        plt.title(f\"Classical RBF Kernel (n={n_components}, size={sample_size})\")\n",
    "        plt.xlabel(\"train idx\")\n",
    "        plt.ylabel(\"train idx\")\n",
    "        plt.colorbar(label=\"Kernel value\")\n",
    "\n",
    "        fname_cl = f\"../Heatmaps/kernel_n{n_components}_size{sample_size}_classical.png\"\n",
    "        plt.savefig(fname_cl, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c01b5",
   "metadata": {},
   "source": [
    "### Results Aggregation and Display\n",
    "Once all runs are complete, this cell converts the collected `results` into a pandas DataFrame, displays the long-format table of metrics, and then pivots it to generate a consolidated accuracy comparison table across PCA dimensions and sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7163f06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoted accuracy table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">classical_accuracy</th>\n",
       "      <th colspan=\"4\" halign=\"left\">quantum_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_size</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>3000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>3000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_components</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.898333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.778333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.911667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.903333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.928333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.896667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.905000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             classical_accuracy                        quantum_accuracy        \\\n",
       "sample_size                10    100    1000      3000             10    100    \n",
       "n_components                                                                    \n",
       "2                           1.0  0.85  0.900  0.898333              1.0  0.80   \n",
       "3                           1.0  0.90  0.905  0.896667              1.0  0.80   \n",
       "4                           1.0  0.90  0.905  0.911667              1.0  0.85   \n",
       "5                           1.0  0.90  0.905  0.908333              1.0  0.85   \n",
       "6                           1.0  0.85  0.905  0.916667              1.0  0.85   \n",
       "7                           1.0  0.90  0.925  0.928333              1.0  0.85   \n",
       "8                           1.0  0.95  0.935  0.935000              1.0  0.85   \n",
       "\n",
       "                               \n",
       "sample_size    1000      3000  \n",
       "n_components                   \n",
       "2             0.780  0.778333  \n",
       "3             0.845  0.840000  \n",
       "4             0.840  0.855000  \n",
       "5             0.860  0.875000  \n",
       "6             0.870  0.903333  \n",
       "7             0.875  0.896667  \n",
       "8             0.875  0.905000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------- Build & Display Results Table -----------\n",
    "df_results = pd.DataFrame(results)                        # Construct a DataFrame from the collected results list\n",
    "\n",
    "#----------- Pivot Into Multi-Level Column Grid -----------\n",
    "df_pivot = df_results.pivot_table(                        # Create a pivot table indexed by n_components and columns by sample_size\n",
    "    index='n_components',\n",
    "    columns='sample_size',\n",
    "    values=['quantum_accuracy','classical_accuracy']      # Include both quantum and classical accuracy values\n",
    ")\n",
    "\n",
    "print(\"Pivoted accuracy table:\")                          # Print header for the pivoted results\n",
    "display(df_pivot)                                         # Render the pivoted accuracy comparison table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
